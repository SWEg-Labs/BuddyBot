1)
Gli ho chiesto:
"Quando √® che √® stato scelto Postgres come database relazionale?"

E mi ha trovato:
- Il glossario nel posto dove si trova la voce "Database relazionale"
- Il documento di candidatura dove √® scritta una data: 22/04/2025

Quindi in realt√† si pu√≤ dire che la similarit√† √® andata bene, facendo una ricerca a singoli pezzi, per√≤ considerando tutta la frase in realt√† la risposta non √® soddisfacente.

E' evidente che bisogna dare all'utente delle istruzioni pi√π precise a riguardo di come porre una domanda.


2)
Gli ho chiesto:
In quale verbale √® stato deciso di utilizzare Postgres come database relazionale?

Mi trova sempre  'candidatura.tex'   e     'src/RTB/Documentazione esterna/Glossario/contenuti/DEF.tex'
Per gli stessi motivi di cui sopra.

E l'assistente giustamente mi risponde:
Non √® possibile trovare informazioni specifiche riguardanti il verbale in cui √® stata presa la decisione di utilizzare Postgres come database relazionale nel contenuto fornito.

Quindi, aggiungere la parola 'verbale' √® inutile, perch√© nel singolo documento 'diario.tex' la parola 'verbale' non √® ovviamente presente.

Per√≤, non si pu√≤ mica chiedere all'utente di scrivere 'cercami in diario.tex' oppure 'cercami in decisioni.tex' ahaah.




Gioved√¨ 26/12/24 :


3)
Gli ho chiesto:
Chi deve redigere la sezione "Valutazioni per il miglioramento" del piano di qualifica?

Mi ha trovato:
La issue giusta con distance: 0.03957073913065037
La issue del cruscotto di valutazione della qualit√†: 0.7241383791087452

Poi ho introdotto la funzione similarity_search_by_threshold con threshold=1.5, 
e mi ha trovato altri 16 risultati, tra 1.0936877952732282 e 1.4810937646072475.
Che sia 1.0 il threshold giusto?


4)
Gli ho chiesto:
Come si chiama il progetto a cui sta lavorando il team?

Con threshold=1.5 trovato 20 documenti, di cui solo i primi 2 sono sotto l'1.0, e ha risposto giusto.

Domanda inadatta, qui il nome del progetto √® in un metadato, quindi basta anche un nome solo.


5)
Gli ho chiesto:
Quali tecnologie devono essere studiate per il progetto?

I primi 2 documenti hanno preso solo la parola "progetto". Quindi con k=2 non avrei avuto risultati
Il primo documento realmente utile, cio√® "Studiare le tecnologie selezionate, con particolare focus su Angular"
ha similarit√† = 0.9129092352621573

"Studiare e approfondire Postgres per il database relazionale" purtroppo ha similarit√† solo 1.0524195594607086.

Giustamente il chatbot mi ha risposto "Angular" e "PostgreSQL", e questo vuol dire che sul chatbot possiamo fare per bene affidamento.
Per√≤, non avrebbe rilevato PostgreSQL se avessi messo 1.0 come soglia.

Come incombent allora adesso mettiamo 1.1 ?


6)
Gli ho chiesto:
Quali requisiti o tipi di requisiti sono da scrivere?

C'√® un "stendere tracciamento fonte-requisito" che ha fatto 1.0154040701727296.
"Stendere requisiti implementativi" ha 1.0253274871704787
"stendere requisiti funzionali" ha 1.0625918189593717
"stendere requisiti prestazionali" ha 1.1076133703782485 mannaggia!!!

Dal successivo 1.1481803063712028 iniziano ad andare a campi, ma fino a 1.107 c'era roba!

Non posso rischiare di escludere, purtroppo, quindi mi tocca alzare l'incombent o a 1.11 o a 1.2.


7)
Gli ho chiesto:
Dimmi cosa c'√® da fare durante le vacanze di Natale

"Incontrarsi, divisi per gruppi, durante le vacanze di Natale, per discutere dei propri casi d'uso" ha 0.5003709322282297.
Per√≤, "Stendere requisiti di vincolo" ha 0.8284832512704778. Diciamo che l'ha vista come una "cosa da fare".

Ho capito che non posso fermare a 1.11 perch√® mi perderei ben quattro cose da fare.
1.2 mi sembra invece azzeccato, perch√® le risposte iniziano a perdere un po' il focus da l√¨ in poi.

Quindi, con solo Jira adesso so che il threshold di similarit√† adatto √® 1.2.

Ho provato, ed in effetti, ok, niente prompt engineering o piano di qualifica, ma comunque ottengo una risposta bella organizzata.

_

Ho riprovato, stavolta usando la funzione "similarity_search_by_threshold_with_gap", e
Mi ha giustamente trovato solo "Incontrarsi, divisi per gruppi, durante le vacanze di Natale, per discutere dei propri casi d'uso",
con il valore di 0.5003709322282297.
Da solo! (Found 1 relevant documents)

E secondo √® giusto cos√¨, io terrei questa funzione, con max_gap = 0.25 di default.


Ora,
DEFINIZIONI:
1) Definizione di cos'√® "fuori contesto" -> 0 documenti trovati
2) Definizione di cos'√® "informazione non trovata" -> il primo documento trovato ha similarit√† > 1.0

Gliele ho scritte nell'header



8)
Chi vince tra un leone e una tigre?

Mi ha trovato "Stendere requisiti di vincolo" (perch√® c'√® "vinc") a 1.0180100370585363.

Giustamente il chatbot risponde "Informazione non trovata.".
Ma, io vorrei rispondesse "domanda fuori contesto".

Allora, ho dovuto rinunciare alla indicazioni oggettive, adesso ho scritto all'LLM di capire da solo
se la domanda √® fuori contesto o meno, quindi mi affido totalmente a lui, sperando ne valga la pena.



9)
Dimmi cosa c'√® da fare durante le vacanze di Pasqua

Uno schifo, purtroppo il primo documento √® "Stendere requisiti di vincolo" a 0.8303134493805799.
L'LLM vorrebbe scrivere "informazione non trovata", ma non pu√≤ perch√® gli ho scritto di scriverlo
solamente quando il primo documento ha similarit√† > 1.0. 
Dunque adesso, beh, per colpa dell'orribile confronto di similarit√†, sono costretto a delegare tutto
il giudizio all'LLM, ed √® un po' un peccato, ma me la metto via.

Mi d√† ancora che la domanda √® fuori contesto. Provo allora a farla entrare meglio nel contesto:


10)
Dimmi cosa c'√® da fare per il progetto BuddyBot durante le vacanze di Pasqua

E mi d√† l'elenco delle attivit√†, anche se non c'√® scritto da nessuna parte "Pasqua"! Mannaggia!
Abbiamo idee diverse io e l'LLM, e questo √® esattamente ci√≤ che temevo!


11)
Dimmi cosa, per il progetto BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Pasqua?

Mi d√† "La domanda √® fuori contesto.", mannaggia!


12)
Dimmi cosa, per il progetto BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Natale?

Risposta perfetta, un unico documento, ed √® quello giusto


13)
Cosa √® previsto che faccia Giacomo Loat a riguardo di Angular?

Adesso dice che deve approfondire anche Angular. Si fa facilmente persuadere l'LLM.
La issue di Angular, per tutti, ha ottenuto "0.9394691898848317".


14)
Per caso Giacomo Loat ha assegnato a suo nome (assignee) qualche compito che riguarda Angular?

Risposta corretta, e fin qui ok, fin l√¨ c'eravamo.



Ripeto la 11, stavolta dico che se viene citato almeno un termine di contesto, la domanda non pu√≤ essere fuori contesto

11)
Dimmi cosa, per il progetto BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Pasqua?

La domanda √® fuori contesto



15)
Dimmi cosa, per il progetto informatico BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Pasqua?

Informazione non trovata.

Godo, alleluia ce l'abbiamo fatta!

üèÜüèÜBisogner√† dire all'utente di descrivere sempre un minimo di contesto della domanda, puntando sulle due parole 
"progetto informatico", cos√¨ almeno GPT4o-mini non ti risponder√† mai "domanda fuori contesto"!üèÜüèÜ

Per oggi ho finito con il trattamento dell'header!



Adesso, introduco GitHub, e vediamo che valori di similarit√† ci trover√≤.

16)
Chi √® l'autore del progetto BrickBreaker?

Mi ha trovato "Terminare il documento di Norme di Progetto" a 1.0324760491732514, dunque
il resto era distante 0.25.
Maledetto Jira!

Riprovo allora solamente con GitHub.

[INFO] Found 0 relevant documents
relevant_docs: []
Assistant: La domanda √® fuori contesto.

Vabb√®, per GitHub serve allora porre domande pi√π specifiche.


17)
Che licenza ha il progetto?

Ancora fuori contesto.


18)
Mi mostri un documento dove √® stato utilizzato il framework di test JUnit?

Fuori contesto.

Per GitHub √® tutto pi√π difficile, il threshold di 1.2 mi sa che non funziona pi√π.

Provo con la funzione vecchia "similarity_search_with_k_results" :

Il file notice √® a 1.25712027786884, mentre BrickBreakerTest.java (il documento risposta giusta) √® a 1.552585526195122.

Fintanto che i file sono cos√¨ lunghi non credo che la similarit√† verr√† mai pi√π lunga di cos√¨, perch√® ovviamente se anche
un pezzo di frase √® molto simile poi comunque ci sono centinaia di altre righe a "distanziare", mentre le issue di Jira sono
cos√¨ corte che per forza di cose la similarit√† viene bassa, perch√® ci sono meno "righe distanzianti".
Molte issues Jira che non c'entrano niente finiscono davanti ai lunghi documenti GitHub che sono invece rilevanti.

üçáPossibilit√†:
1) Imponiamo sempre uno alla volta, o solo GitHub e Jira. E' INAMMISSIBILE, CI E' STATO CHIARAMENTE DETTO CHE IL DATABASE VETTORIALE
DEVE CONTENERE ENTRAMBI.
2) Dividiamo i documenti GitHub in documentini lunghi come le issue Jira. EH, MA SPEZZANDO TROPPO SI PERDE LA VISIONE D'INSIEME 
DEL DOCUMENTO, E ALLORA NON CI TRAI PIU' INFORMAZIONI.
3) Diamo un peso alla similarit√† di GitHub perch√® sia pi√π pesante della similarit√† di Jira. NO, PERCHE' UNA VOLTA CHE SONO NEL 
DATABASE VETTORIALE NON E' PIU' POSSIBILE DISTINGUERE L'ORIGINE DEI DOCUMENTI. IN REALTA', USANDO I METADATI SI PUO', PERO'
ESISTONO ANCHE DOCUMENTI PROVENIENTI DA GITHUB CHE SONO CORTI, E QUINDI LORO USCIREBBERO FUORI SEMPRE!
4) Togliere i tag a Confluence, cos√¨ accorcio il suo testo. OK, MA, C'E', MITIGA UN PO' IL PROBLEMA MA NON LO RISOLVE IN SENSO GENERALE


19)
Qual √® la data dell'ultimo commit di BrickBreaker?

BrickBreakerTest.java : 1.6413434730712964
README.md : 1.6453533610747808

"Informazione non trovata."
Godo, ho istruito il chatbot bene, ho fatto un buon lavoro!

Usando il threshold invece ovviamente (visto che non trova documenti) mi d√†:

[INFO] Found 0 relevant documents
relevant_docs: []
Assistant: La domanda √® fuori contesto.

E' il solito discorso, la similarit√† dei documenti di GitHub √® molto pi√π alta, e quel threshold non √® pi√π adatto.
Confermo dunque quanto gi√† scritto.



Passo a Confluence per vedere su che valori si aggira la similarit√† sulle sue pagine.
In teoria, siccome contengono tanto testo, purtroppo anche da loro mi aspetto di incontrare una similarit√† alta, purtroppo!

20) Come si chiama il progetto a cui sta lavorando il team?

Found 1 relevant documents
'title': 'Sprint Retrospettiva: Il Progetto "Connettivit√† per Tutti"', 'distance': 1.0568217419342338

Ha funzionato perfettamente! Sono contento di essere rimasto dentro al threshold, per√≤ sono perfettamente
consapevole che Jira avrebbe fatto meglio e avrebbe battuto questo documento.
Provo ad aggiungere Jira:
Confermo, come immaginavo.
_
Found 7 relevant documents
Studiare e approfondire Postgres per il database relazionale : 0.9797200954101462
Terminare il documento di Norme di Progetto : 0.9896156543847999
'title': 'Sprint Retrospettiva: Il Progetto "Connettivit√† per Tutti"', 'distance': 1.0568217419342338
_
Confluence √® arrivato terzo, per carit√†, buon risultato, ma l'LLM ha risposto solo con BuddyBot, il che vuol dire che
Confluence √® stato oscurato pur essendo terzo!


21) 
Per quale progetto √® stata svolta di recente la Retrospettiva di una sprint?
_
Found 14 relevant documents
'distance': 0.9797555929844596}\nContent: Terminare il documento di Norme di Progetto"
'distance': 0.9977851255568352}\nContent: Stendere requisiti di vincolo"
'title': 'Sprint Retrospettiva: Il Progetto "Connettivit√† per Tutti"', 'distance': 1.0090792822755426
_
Confluence aveva perso un'altra volta! Per fortuna l'LLM √® intelligente e ha saputo leggere i dati e dare la risposta giusta.
Per√≤ il discorso √® che questa volta Confluence non avrebbe dovuto perdere.
Non √® nemmeno riuscita a scendere sotto l'1.0 !!
E questa √® la prova definitiva che pi√π un documento √® lungo peggio sta messa la sua similarit√†. Sad.
Prima avevo azzeccato il problema, e niente, adesso devo pensare a quelle 4 possibilit√†üçá.
_
SONO RIUSCITO A RISOLVERE QUESTO PROBLEMA RIMUOVENDO I TAG A CONFLUENCE, SI PUO' VEDERE QUA SOTTO!





Sabato 28/12/24


Riscrivo le proposte e le mie considerazioni aggiornate:

üçáPossibilit√†:
1) Imponiamo sempre uno alla volta, o solo GitHub e Jira. E' INAMMISSIBILE, CI E' STATO CHIARAMENTE DETTO CHE IL DATABASE VETTORIALE
DEVE CONTENERE ENTRAMBI CONTEMPORANEAMENTE.
2) Dividiamo i documenti GitHub in documentini lunghi come le issue Jira. EH, MA SPEZZANDO TROPPO SI PERDE LA VISIONE D'INSIEME 
DEL DOCUMENTO, E ALLORA NON CI TRAI PIU' INFORMAZIONI. La stessa similarit√† verrebbe falsata cos8.
3) Diamo un peso alla similarit√† di GitHub perch√® sia pi√π pesante della similarit√† di Jira. ‚ùå{NO, PERCHE' UNA VOLTA CHE SONO NEL 
DATABASE VETTORIALE NON E' PIU' POSSIBILE DISTINGUERE L'ORIGINE DEI DOCUMENTI. [IN REALTA', USANDO I METADATI SI PUO']}‚ùå, PERO'
ESISTONO ANCHE DOCUMENTI PROVENIENTI DA GITHUB CHE SONO CORTI, E QUINDI LORO USCIREBBERO FUORI SEMPRE perch√© hanno la similarit√† che sarebbe gi√† bassa di suo e poi se gli dai pure maggior peso anche allora viene veramente troppo bassa!
4) Togliere i tag a Confluence, cos√¨ accorcio il suo testo. OK, MA, C'E', QUESTO MITIGA UN PO' IL PROBLEMA MA NON LO RISOLVE IN SENSO GENERALE.
5) Dovremo accettare che le issues Jira siano pi√π simili. E allora, bisogna alzare il threshold perch√© vengano mostrati all'LLM anche documenti provenienti da GitHub, poco da fare. MA COS√å ALLORA OGNI VOLTA VERREBBERO FORNITE AL CHATBOT TUTTE LE ISSUES (perch√© tutte o quasi tutte entrerebbero nel range), CHE VUOL DIRE UTILIZZARE TANTISSIMI TOKEN!


L'unica soluzione effettivamente realizzabile √® la 4, cio√® rimuovere i tag HTML da Confluence, che pu√≤ mitigare un po'.
Le altre hanno tutte troppi punti deboli.

Come threshold, io terrei 1.2 per il momento, perch√© comunque √® importante fare risparmio di token, √® l'obiettivo primario della similarit√†.


Ok, ho fatto le funzioni di strip dei tag di Confluence.



E adesso riprovo la domanda 21 gi√† provata sopra quando c'erano i tag:

21) 
Per quale progetto √® stata svolta di recente la Retrospettiva di una sprint?

You: Per quale progetto √® stata svolta di recente la Retrospettiva di una sprint?
[INFO] Found 1 relevant documents
\'title\': \'Sprint Retrospettiva: Il Progetto "Connettivit√† per Tutti"\', \'distance\': 0.8602511921421436

Ho migliorato di 0,1966, in pratica 0,2, cio√® di 2 decimi di unit√†!
Un eccellente miglioramento, tale per cui sono contento di aver tolto i tag.

(ottimo anche il fatto che abbia funzionato il max_gap di 0.3).

Adesso riprovo dopo aver caricato anche Jira:

[INFO] Found 14 relevant documents
Mannaggia!

'title': 'Sprint Retrospettiva: Il Progetto "Connettivit√† per Tutti"', 'distance': 0.8602511921421436
Per fortuna ha vinto Confluence!

Poi:
'distance': 0.9797555929844596}\nContent: Terminare il documento di Norme di Progetto"
'distance': 0.9977851255568352}\nContent: Stendere requisiti di vincolo"
[e le altre]

(Il chatbot per fortuna ha risposto correttamente)

Adesso √® stato risolto il problema della domanda 21, sono contento!







29/12/24

AzzurroDigitale ci aveva chiesto di implementare le seguenti funzionalit√† da GitHub:

1) Sia il codice
2) Sia quando √® stato scritto
3) Sia chi l'ha scritto

Di conseguenza, la prima cosa da fare √® implementare una funzione get_commits(self, owner, repo_name).

Successivamente, devo anche aggiungere una funzione alla classe VectorStoreRepository perch√® vada ad 
aggiungere i commit allo store. C'√® per√≤ un design smell, non √® accettabile che ogni volta si debba
creare una nuova funzione per poter aggiungere un nuovo elemento GitHub. Sar√† da risolvere in fase
di design, servir√† inserire un middleware di conversione dati, per poter ottenere delle classi pi√π
piccole, non il blob di codice che √® diventato VectorStoreRepository! Questo middleware si chiama Adapter!
Ecco un nuovo design pattern che possiamo utilizzare, oltre allo Strategy!
_
Comunque, al momento aggiungo i due metodi add_github_commits(self, commits) e _split_github_commits(self, commits), 
perch√® non siamo ancora nella PB, non √® ancora tempo di design.

E, ovviamente, c'√® da modificare correttamente la gestione dell'input.

Ok, implementato il tutto, adesso, qualche domanda:
