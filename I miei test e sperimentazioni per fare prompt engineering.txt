1)
Gli ho chiesto:
"Quando √® che √® stato scelto Postgres come database relazionale?"

E mi ha trovato:
- Il glossario nel posto dove si trova la voce "Database relazionale"
- Il documento di candidatura dove √® scritta una data: 22/04/2025

Quindi in realt√† si pu√≤ dire che la similarit√† √® andata bene, facendo una ricerca a singoli pezzi, per√≤ considerando tutta la frase in realt√† la risposta non √® soddisfacente.

E' evidente che bisogna dare all'utente delle istruzioni pi√π precise a riguardo di come porre una domanda.


2)
Gli ho chiesto:
In quale verbale √® stato deciso di utilizzare Postgres come database relazionale?

Mi trova sempre  'candidatura.tex'   e     'src/RTB/Documentazione esterna/Glossario/contenuti/DEF.tex'
Per gli stessi motivi di cui sopra.

E l'assistente giustamente mi risponde:
Non √® possibile trovare informazioni specifiche riguardanti il verbale in cui √® stata presa la decisione di utilizzare Postgres come database relazionale nel contenuto fornito.

Quindi, aggiungere la parola 'verbale' √® inutile, perch√© nel singolo documento 'diario.tex' la parola 'verbale' non √® ovviamente presente.

Per√≤, non si pu√≤ mica chiedere all'utente di scrivere 'cercami in diario.tex' oppure 'cercami in decisioni.tex' ahaah.



3)
Gli ho chiesto:
Chi deve redigere la sezione "Valutazioni per il miglioramento" del piano di qualifica?

Mi ha trovato:
La issue giusta con distance: 0.03957073913065037
La issue del cruscotto di valutazione della qualit√†: 0.7241383791087452

Poi ho introdotto la funzione similarity_search_by_threshold con threshold=1.5, 
e mi ha trovato altri 16 risultati, tra 1.0936877952732282 e 1.4810937646072475.
Che sia 1.0 il threshold giusto?


4)
Gli ho chiesto:
Come si chiama il progetto a cui sta lavorando il team?

Con threshold=1.5 trovato 20 documenti, di cui solo i primi 2 sono sotto l'1.0, e ha risposto giusto.

Domanda inadatta, qui il nome del progetto √® in un metadato, quindi basta anche un nome solo.


5)
Gli ho chiesto:
Quali tecnologie devono essere studiate per il progetto?

I primi 2 documenti hanno preso solo la parola "progetto". Quindi con k=2 non avrei avuto risultati
Il primo documento realmente utile, cio√® "Studiare le tecnologie selezionate, con particolare focus su Angular"
ha similarit√† = 0.9129092352621573

"Studiare e approfondire Postgres per il database relazionale" purtroppo ha similarit√† solo 1.0524195594607086.

Giustamente il chatbot mi ha risposto "Angular" e "PostgreSQL", e questo vuol dire che sul chatbot possiamo fare per bene affidamento.
Per√≤, non avrebbe rilevato PostgreSQL se avessi messo 1.0 come soglia.

Come incombent allora adesso mettiamo 1.1 ?


6)
Gli ho chiesto:
Quali requisiti o tipi di requisiti sono da scrivere?

C'√® un "stendere tracciamento fonte-requisito" che ha fatto 1.0154040701727296.
"Stendere requisiti implementativi" ha 1.0253274871704787
"stendere requisiti funzionali" ha 1.0625918189593717
"stendere requisiti prestazionali" ha 1.1076133703782485 mannaggia!!!

Dal successivo 1.1481803063712028 iniziano ad andare a campi, ma fino a 1.107 c'era roba!

Non posso rischiare di escludere, purtroppo, quindi mi tocca alzare l'incombent o a 1.11 o a 1.2.


7)
Gli ho chiesto:
Dimmi cosa c'√® da fare durante le vacanze di Natale

"Incontrarsi, divisi per gruppi, durante le vacanze di Natale, per discutere dei propri casi d'uso" ha 0.5003709322282297.
Per√≤, "Stendere requisiti di vincolo" ha 0.8284832512704778. Diciamo che l'ha vista come una "cosa da fare".

Ho capito che non posso fermare a 1.11 perch√® mi perderei ben quattro cose da fare.
1.2 mi sembra invece azzeccato, perch√® le risposte iniziano a perdere un po' il focus da l√¨ in poi.

Quindi, con solo Jira adesso so che il threshold di similarit√† adatto √® 1.2.

Ho provato, ed in effetti, ok, niente prompt engineering o piano di qualifica, ma comunque ottengo una risposta bella organizzata.

_

Ho riprovato, stavolta usando la funzione "similarity_search_by_threshold_with_gap", e
Mi ha giustamente trovato solo "Incontrarsi, divisi per gruppi, durante le vacanze di Natale, per discutere dei propri casi d'uso",
con il valore di 0.5003709322282297.
Da solo! (Found 1 relevant documents)

E secondo √® giusto cos√¨, io terrei questa funzione, con max_gap = 0.25 di default.


Ora,
DEFINIZIONI:
1) Definizione di cos'√® "fuori contesto" -> 0 documenti trovati
2) Definizione di cos'√® "informazione non trovata" -> il primo documento trovato ha similarit√† > 1.0

Gliele ho scritte nell'header



8)
Chi vince tra un leone e una tigre?

Mi ha trovato "Stendere requisiti di vincolo" (perch√® c'√® "vinc") a 1.0180100370585363.

Giustamente il chatbot risponde "Informazione non trovata.".
Ma, io vorrei rispondesse "domanda fuori contesto".

Allora, ho dovuto rinunciare alla indicazioni oggettive, adesso ho scritto all'LLM di capire da solo
se la domanda √® fuori contesto o meno, quindi mi affido totalmente a lui, sperando ne valga la pena.



9)
Dimmi cosa c'√® da fare durante le vacanze di Pasqua

Uno schifo, purtroppo il primo documento √® "Stendere requisiti di vincolo" a 0.8303134493805799.
L'LLM vorrebbe scrivere "informazione non trovata", ma non pu√≤ perch√® gli ho scritto di scriverlo
solamente quando il primo documento ha similarit√† > 1.0. 
Dunque adesso, beh, per colpa dell'orribile confronto di similarit√†, sono costretto a delegare tutto
il giudizio all'LLM, ed √® un po' un peccato, ma me la metto via.

Mi d√† ancora che la domanda √® fuori contesto. Provo allora a farla entrare meglio nel contesto:


10)
Dimmi cosa c'√® da fare per il progetto BuddyBot durante le vacanze di Pasqua

E mi d√† l'elenco delle attivit√†, anche se non c'√® scritto da nessuna parte "Pasqua"! Mannaggia!
Abbiamo idee diverse io e l'LLM, e questo √® esattamente ci√≤ che temevo!


11)
Dimmi cosa, per il progetto BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Pasqua?

Mi d√† "La domanda √® fuori contesto.", mannaggia!


12)
Dimmi cosa, per il progetto BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Natale?

Risposta perfetta, un unico documento, ed √® quello giusto


13)
Cosa √® previsto che faccia Giacomo Loat a riguardo di Angular?

Adesso dice che deve approfondire anche Angular. Si fa facilmente persuadere l'LLM.
La issue di Angular, per tutti, ha ottenuto "0.9394691898848317".


14)
Per caso Giacomo Loat ha assegnato a suo nome (assignee) qualche compito che riguarda Angular?

Risposta corretta, e fin qui ok, fin l√¨ c'eravamo.



Ripeto la 11, stavolta dico che se viene citato almeno un termine di contesto, la domanda non pu√≤ essere fuori contesto

11)
Dimmi cosa, per il progetto BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Pasqua?

La domanda √® fuori contesto



15)
Dimmi cosa, per il progetto informatico BuddyBot, c'√® scritto espressamente che √® da fare durante le vacanze di Pasqua?

Informazione non trovata.

Godo, alleluia ce l'abbiamo fatta!

üèÜüèÜBisogner√† dire all'utente di descrivere sempre un minimo di contesto della domanda, puntando sulle due parole 
"progetto informatico", cos√¨ almeno GPT4o-mini non ti risponder√† mai "domanda fuori contesto"!üèÜüèÜ

Per oggi ho finito con il trattamento dell'header!